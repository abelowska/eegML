{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abelowska/eegML/blob/main/Classes_07_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40a73099-38c1-4bf0-afd3-5481df7c3d19",
      "metadata": {
        "id": "40a73099-38c1-4bf0-afd3-5481df7c3d19"
      },
      "source": [
        "# Prediction of perfectionism CMDA subscale from error-related negativity ERP component"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we're going to play with different pipelines - and thus we're going to implement various `transformers`, i.e., pipeliens steps to transform data."
      ],
      "metadata": {
        "id": "k5trA0_vAvKo"
      },
      "id": "k5trA0_vAvKo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install additional libraries"
      ],
      "metadata": {
        "id": "scBXb1bvEzGA"
      },
      "id": "scBXb1bvEzGA"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install MNE"
      ],
      "metadata": {
        "id": "EIW4QA1REv8X"
      },
      "id": "EIW4QA1REv8X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d8ff9f56-5483-44c7-b9cc-486a31838279",
      "metadata": {
        "tags": [],
        "id": "d8ff9f56-5483-44c7-b9cc-486a31838279"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e9e84d26-b32c-41ae-b3f4-81e0ee55156b",
      "metadata": {
        "id": "e9e84d26-b32c-41ae-b3f4-81e0ee55156b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "import ast\n",
        "import os.path as op\n",
        "from collections import defaultdict\n",
        "from copy import deepcopy\n",
        "import copy\n",
        "\n",
        "import pickle\n",
        "import mne\n",
        "import scipy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import set_config\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.feature_selection import SelectKBest, SelectPercentile, f_regression, SelectFpr, SelectFdr, SequentialFeatureSelector\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "from sklearn import datasets, linear_model\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\", palette=\"deep\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59c69899-6a32-4418-9079-8bd66aed497f",
      "metadata": {
        "tags": [],
        "id": "59c69899-6a32-4418-9079-8bd66aed497f"
      },
      "source": [
        "---\n",
        "## Load data\n",
        "\n",
        "Loading EEG and questionnaire data. By default create_df_data loads all info from given .csv file but one can specify it by passing a list of desired labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7c412f36-66a2-4fd8-bfa7-caa98c4a315c",
      "metadata": {
        "id": "7c412f36-66a2-4fd8-bfa7-caa98c4a315c"
      },
      "outputs": [],
      "source": [
        "def create_df_data(\n",
        "    dir_path,\n",
        "    info_filename=None,\n",
        "    info=\"all\",\n",
        "):\n",
        "    \"\"\"Loads data for all participants and create DataFrame with optional additional info from given .csv file.\n",
        "\n",
        "    On default, loads a train set: chooses only 80% of participants\n",
        "    and for each of them chooses 80% of epochs.\n",
        "    It will choose them deterministically.\n",
        "\n",
        "    If test_participants is set to True, it will load remaining 20% of participants.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    test: bool\n",
        "        whether load data for training or final testing.\n",
        "        If true load participants data for testing.\n",
        "    info_filename: String | None\n",
        "        path to .csv file with additional data.\n",
        "    info: array\n",
        "        listed parameters from the info file to be loaded.\n",
        "        if 'all', load all parameters\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    go_nogo_data_df : pandas.DataFrame\n",
        "\n",
        "    \"\"\"\n",
        "    print(dir_path)\n",
        "    header_files_glob = \"responses_100_600/*.vhdr\"\n",
        "    print(header_files_glob)\n",
        "\n",
        "    # extract header files\n",
        "    header_files = glob.glob(header_files_glob)\n",
        "    header_files = sorted(header_files)\n",
        "    print(header_files)\n",
        "\n",
        "    # create dataframe for results\n",
        "    go_nogo_data_df = pd.DataFrame()\n",
        "\n",
        "    for file in header_files:\n",
        "        # load eeg data for given participant\n",
        "        participant_epochs = load_epochs_from_file(file)\n",
        "\n",
        "        # and compute participant's id from file_name\n",
        "        participant_id = re.match(r\".*_(\\w+).*\", file).group(1)\n",
        "\n",
        "        error = participant_epochs[\"error_response\"]._data\n",
        "        correct = participant_epochs[\"correct_response\"]._data\n",
        "\n",
        "        # exclude those participants who have too few samples\n",
        "        if len(error) < 5 or len(correct) < 5:\n",
        "            # not enough data for this participant\n",
        "            continue\n",
        "\n",
        "        # construct dataframe for participant with: id|epoch_data|response_type|additional info...\n",
        "        participant_df = create_df_from_epochs(\n",
        "            participant_id, participant_epochs, info_filename, info\n",
        "        )\n",
        "\n",
        "        # add participant's data to results dataframe\n",
        "        print(participant_id)\n",
        "        go_nogo_data_df = go_nogo_data_df.append(participant_df, ignore_index=True)\n",
        "\n",
        "    return go_nogo_data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "aa88517e-884a-4b5b-9652-1bf2e7787aef",
      "metadata": {
        "id": "aa88517e-884a-4b5b-9652-1bf2e7787aef"
      },
      "outputs": [],
      "source": [
        "def load_epochs_from_file(file, reject_bad_segments=\"auto\", mask=None):\n",
        "    \"\"\"Load epochs from a header file.\n",
        "\n",
        "    Args:\n",
        "        file: path to a header file (.vhdr)\n",
        "        reject_bad_segments: 'auto' means that bad segments are rejected automatically.\n",
        "\n",
        "    Returns:\n",
        "        mne Epochs\n",
        "\n",
        "    \"\"\"\n",
        "    # Import the BrainVision data into an MNE Raw object\n",
        "    raw = mne.io.read_raw_brainvision(file)\n",
        "\n",
        "    # Construct annotation filename\n",
        "    annot_file = file[:-4] + \"vmrk\"\n",
        "\n",
        "    # Read in the event information as MNE annotations\n",
        "    annotations = mne.read_annotations(annot_file)\n",
        "\n",
        "    # Add the annotations to our raw object so we can use them with the data\n",
        "    raw.set_annotations(annotations)\n",
        "\n",
        "    # Map with response markers only\n",
        "    event_dict = {\n",
        "        \"Stimulus/RE*ex*1_n*1_c_1*R*FB\": 10004,\n",
        "        \"Stimulus/RE*ex*1_n*1_c_1*R*FG\": 10005,\n",
        "        \"Stimulus/RE*ex*1_n*1_c_2*R\": 10006,\n",
        "        \"Stimulus/RE*ex*1_n*2_c_1*R\": 10007,\n",
        "        \"Stimulus/RE*ex*2_n*1_c_1*R\": 10008,\n",
        "        \"Stimulus/RE*ex*2_n*2_c_1*R*FB\": 10009,\n",
        "        \"Stimulus/RE*ex*2_n*2_c_1*R*FG\": 10010,\n",
        "        \"Stimulus/RE*ex*2_n*2_c_2*R\": 10011,\n",
        "    }\n",
        "\n",
        "    # Map for merged correct/error response markers\n",
        "    merged_event_dict = {\"correct_response\": 0, \"error_response\": 1}\n",
        "\n",
        "    # Reconstruct the original events from Raw object\n",
        "    events, event_ids = mne.events_from_annotations(raw, event_id=event_dict)\n",
        "\n",
        "    # Merge correct/error response events\n",
        "    merged_events = mne.merge_events(\n",
        "        events,\n",
        "        [10004, 10005, 10009, 10010],\n",
        "        merged_event_dict[\"correct_response\"],\n",
        "        replace_events=True,\n",
        "    )\n",
        "    merged_events = mne.merge_events(\n",
        "        merged_events,\n",
        "        [10006, 10007, 10008, 10011],\n",
        "        merged_event_dict[\"error_response\"],\n",
        "        replace_events=True,\n",
        "    )\n",
        "\n",
        "    # epochs = []\n",
        "    this_reject_by_annotation = True\n",
        "\n",
        "    # Read epochs\n",
        "    epochs = mne.Epochs(\n",
        "        raw=raw,\n",
        "        events=merged_events,\n",
        "        event_id=merged_event_dict,\n",
        "        tmin=tmin,\n",
        "        tmax=tmax,\n",
        "        baseline=None,\n",
        "        reject_by_annotation=this_reject_by_annotation,\n",
        "        preload=True,\n",
        "        # verbose='CRITICAL',\n",
        "    )\n",
        "    \n",
        "    return epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0991419b-7acd-42bc-b048-e5e9fa3cb3e6",
      "metadata": {
        "id": "0991419b-7acd-42bc-b048-e5e9fa3cb3e6"
      },
      "outputs": [],
      "source": [
        "def create_df_from_epochs(\n",
        "    id, \n",
        "    participant_epochs, \n",
        "    info_filename, \n",
        "    info\n",
        "):\n",
        "    \"\"\"Create df for each participant. DF structure is like: {id: String ; epoch: epoch_data ; marker: 1.0|0.0}\n",
        "    1.0 means correct and 0.0 means error response.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    id: String\n",
        "        participant's id extracted from filename\n",
        "    participant_epochs: mne Epochs\n",
        "        epoched eeg data\n",
        "    info_filename: String\n",
        "        path to .csv file with questionnaire data.\n",
        "    info: array\n",
        "        listed parameters from the info file to be loaded.\n",
        "        if 'all', load all parameters\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    participant_df : pandas.DataFrame\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # create dataframe for participant's questionnaire data\n",
        "    info_df = pd.DataFrame()\n",
        "\n",
        "    # extract questionnaire data from .csv file\n",
        "    if info_filename is not None:\n",
        "        if info == \"all\":\n",
        "            this_info_df = pd.read_csv(info_filename)\n",
        "        else:\n",
        "            this_info_df = pd.read_csv(info_filename, usecols=[\"Demo_kod\"] + info)\n",
        "        info_df = (\n",
        "            this_info_df.loc[this_info_df[\"Demo_kod\"] == id]\n",
        "            .reset_index()\n",
        "            .drop(\"index\", axis=1)\n",
        "        )\n",
        "        \n",
        "    # create dataframe record with participant's data: ID, eeg data, questionnaire data    \n",
        "    participant_df = pd.DataFrame(\n",
        "        {\n",
        "            \"id\": [id], \n",
        "            \"epoch\": [participant_epochs], \n",
        "        }).join(\n",
        "            info_df\n",
        "        )\n",
        "\n",
        "    return participant_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2db5e07d-9618-47de-a706-687a529fb2fe",
      "metadata": {
        "id": "2db5e07d-9618-47de-a706-687a529fb2fe"
      },
      "source": [
        "### Read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "27a25389-d030-415b-982d-b68df981b758"
      },
      "outputs": [],
      "source": [
        "# constants\n",
        "tmin, tmax = -0.101562, 0.5937525  # Start and end of the segments\n",
        "signal_frequency = 256\n",
        "random_state = 42\n",
        "test_size = 0.3\n",
        "\n",
        "ERROR = 1\n",
        "CORRECT = 0"
      ],
      "id": "27a25389-d030-415b-982d-b68df981b758"
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive in colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "JWiBXAsRJmeY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae8edfe3-24c1-4b04-923b-27ede8b98fc4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "id": "JWiBXAsRJmeY"
    },
    {
      "cell_type": "code",
      "source": [
        "# display data in folder\n",
        "!ls gdrive/MyDrive/perfectionism_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiggyQrRJ_YL",
        "outputId": "9ae80860-30fd-4cc1-a59f-18385760914e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'GNG_perfectionism (1).pkl'  'GNG_perfectionism (6).pkl'\n",
            "'GNG_perfectionism (2).pkl'   GNG_perfectionism.pkl\n",
            "'GNG_perfectionism (3).pkl'   picked\n",
            "'GNG_perfectionism (4).pkl'   responses_100_600.zip\n",
            "'GNG_perfectionism (5).pkl'   scales\n"
          ]
        }
      ],
      "id": "yiggyQrRJ_YL"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9db8c673-5477-4126-b09d-870a8fcd9477",
      "metadata": {
        "id": "9db8c673-5477-4126-b09d-870a8fcd9477",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaf3da28-d5e0-4699-d662-a99ea3b6e6f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pickled file found. Loading pickled data...\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "# paths to data\n",
        "dir_path = \"gdrive/MyDrive/perfectionism_data/\"\n",
        "questionnaire_filename = dir_path + \"scales/all_scales.csv\"\n",
        "\n",
        "# define dataframe name in a way: TASK_questionnaire\n",
        "df_name = \"GNG_perfectionism\"\n",
        "\n",
        "# check whether pickled data exists\n",
        "pickled_data_filename = dir_path + df_name + \".pkl\"\n",
        "\n",
        "if os.path.isfile(pickled_data_filename):\n",
        "    print(\"Pickled file found. Loading pickled data...\")\n",
        "    epochs_df = pd.read_pickle(pickled_data_filename)\n",
        "    print(\"Done\")\n",
        "    pass\n",
        "else:\n",
        "    print(\"Pickled file not found. Loading data...\")\n",
        "    epochs_df = create_df_data(\n",
        "        dir_path = dir_path,\n",
        "        info_filename=questionnaire_filename,\n",
        "        info=\"all\"\n",
        "    )\n",
        "\n",
        "    epochs_df.name = df_name\n",
        "    \n",
        "    # save loaded data into a pickle file\n",
        "    epochs_df.to_pickle(dir_path + epochs_df.name + \".pkl\")\n",
        "    print(\"Done. Pickle file created\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(epochs_df.shape)\n",
        "epochs_df.head()"
      ],
      "metadata": {
        "id": "Z8EvPrqJiLpB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        },
        "outputId": "8231affd-8fe3-4908-f375-7bc85021f234"
      },
      "id": "Z8EvPrqJiLpB",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(138, 11)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       id                                              epoch Demo_kod  \\\n",
              "0  AA0303  <Epochs |  201 events (all good), -0.101562 - ...   AA0303   \n",
              "1  AB0601  <Epochs |  221 events (all good), -0.101562 - ...   AB0601   \n",
              "2  AB0612  <Epochs |  253 events (all good), -0.101562 - ...   AB0612   \n",
              "3  AC2011  <Epochs |  173 events (all good), -0.101562 - ...   AC2011   \n",
              "4  AD1308  <Epochs |  202 events (all good), -0.101562 - ...   AD1308   \n",
              "\n",
              "   17-Perfect CM-Concern over Mistakes (9 items mean)  \\\n",
              "0                                               2.22    \n",
              "1                                               1.78    \n",
              "2                                               2.56    \n",
              "3                                               1.67    \n",
              "4                                               4.22    \n",
              "\n",
              "   17-Perfect PS-Personal Standards (7 items mean)  \\\n",
              "0                                             2.43   \n",
              "1                                             2.14   \n",
              "2                                             1.86   \n",
              "3                                             4.57   \n",
              "4                                             4.86   \n",
              "\n",
              "   17-Perfect PE-Parental Expectations (5 items mean)  \\\n",
              "0                                                1.8    \n",
              "1                                                1.8    \n",
              "2                                                1.4    \n",
              "3                                                1.2    \n",
              "4                                                3.4    \n",
              "\n",
              "   17-Perfect PC=Parental Criticism (4 items mean)  \\\n",
              "0                                             1.50   \n",
              "1                                             1.75   \n",
              "2                                             2.25   \n",
              "3                                             1.75   \n",
              "4                                             2.50   \n",
              "\n",
              "   17-Perfect D=Doubts about Actions (4 items mean)  \\\n",
              "0                                              2.75   \n",
              "1                                              2.00   \n",
              "2                                              2.50   \n",
              "3                                              3.75   \n",
              "4                                              4.50   \n",
              "\n",
              "   17-Perfect O=Organization (6 items mean)  \\\n",
              "0                                      3.83   \n",
              "1                                      3.33   \n",
              "2                                      4.00   \n",
              "3                                      5.00   \n",
              "4                                      3.67   \n",
              "\n",
              "   17-Perfectionism full scale (mean)  17-Perfectionism CMDA  \n",
              "0                                2.46                   4.97  \n",
              "1                                2.14                   3.78  \n",
              "2                                2.46                   5.06  \n",
              "3                                3.00                   5.42  \n",
              "4                                3.97                   8.72  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f1be27ec-154f-4efe-8604-7776a67e51c0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>epoch</th>\n",
              "      <th>Demo_kod</th>\n",
              "      <th>17-Perfect CM-Concern over Mistakes (9 items mean)</th>\n",
              "      <th>17-Perfect PS-Personal Standards (7 items mean)</th>\n",
              "      <th>17-Perfect PE-Parental Expectations (5 items mean)</th>\n",
              "      <th>17-Perfect PC=Parental Criticism (4 items mean)</th>\n",
              "      <th>17-Perfect D=Doubts about Actions (4 items mean)</th>\n",
              "      <th>17-Perfect O=Organization (6 items mean)</th>\n",
              "      <th>17-Perfectionism full scale (mean)</th>\n",
              "      <th>17-Perfectionism CMDA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AA0303</td>\n",
              "      <td>&lt;Epochs |  201 events (all good), -0.101562 - ...</td>\n",
              "      <td>AA0303</td>\n",
              "      <td>2.22</td>\n",
              "      <td>2.43</td>\n",
              "      <td>1.8</td>\n",
              "      <td>1.50</td>\n",
              "      <td>2.75</td>\n",
              "      <td>3.83</td>\n",
              "      <td>2.46</td>\n",
              "      <td>4.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AB0601</td>\n",
              "      <td>&lt;Epochs |  221 events (all good), -0.101562 - ...</td>\n",
              "      <td>AB0601</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>1.8</td>\n",
              "      <td>1.75</td>\n",
              "      <td>2.00</td>\n",
              "      <td>3.33</td>\n",
              "      <td>2.14</td>\n",
              "      <td>3.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AB0612</td>\n",
              "      <td>&lt;Epochs |  253 events (all good), -0.101562 - ...</td>\n",
              "      <td>AB0612</td>\n",
              "      <td>2.56</td>\n",
              "      <td>1.86</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2.25</td>\n",
              "      <td>2.50</td>\n",
              "      <td>4.00</td>\n",
              "      <td>2.46</td>\n",
              "      <td>5.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AC2011</td>\n",
              "      <td>&lt;Epochs |  173 events (all good), -0.101562 - ...</td>\n",
              "      <td>AC2011</td>\n",
              "      <td>1.67</td>\n",
              "      <td>4.57</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.75</td>\n",
              "      <td>3.75</td>\n",
              "      <td>5.00</td>\n",
              "      <td>3.00</td>\n",
              "      <td>5.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AD1308</td>\n",
              "      <td>&lt;Epochs |  202 events (all good), -0.101562 - ...</td>\n",
              "      <td>AD1308</td>\n",
              "      <td>4.22</td>\n",
              "      <td>4.86</td>\n",
              "      <td>3.4</td>\n",
              "      <td>2.50</td>\n",
              "      <td>4.50</td>\n",
              "      <td>3.67</td>\n",
              "      <td>3.97</td>\n",
              "      <td>8.72</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f1be27ec-154f-4efe-8604-7776a67e51c0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f1be27ec-154f-4efe-8604-7776a67e51c0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f1be27ec-154f-4efe-8604-7776a67e51c0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare data"
      ],
      "metadata": {
        "id": "_-x3hDhdUncr"
      },
      "id": "_-x3hDhdUncr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check for Nans"
      ],
      "metadata": {
        "id": "mu8C_fA83JJR"
      },
      "id": "mu8C_fA83JJR"
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_df.isnull().sum()"
      ],
      "metadata": {
        "id": "HYAMXqDW3Iai"
      },
      "id": "HYAMXqDW3Iai",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_df.fillna(epochs_df.mean(), inplace=True)\n",
        "epochs_df.isnull().sum()"
      ],
      "metadata": {
        "id": "A0rjTonp36le"
      },
      "id": "A0rjTonp36le",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create X and y sets"
      ],
      "metadata": {
        "id": "25Lvnxt44oek"
      },
      "id": "25Lvnxt44oek"
    },
    {
      "cell_type": "code",
      "source": [
        "X = epochs_df[['epoch']]\n",
        "y = epochs_df[['17-Perfectionism CMDA']].to_numpy().ravel()"
      ],
      "metadata": {
        "id": "y5uiXogV49Ff"
      },
      "id": "y5uiXogV49Ff",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-test split"
      ],
      "metadata": {
        "id": "x-rmifjK2zIv"
      },
      "id": "x-rmifjK2zIv"
    },
    {
      "cell_type": "code",
      "source": [
        "# train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, \n",
        "    y, \n",
        "    test_size=test_size, \n",
        "    random_state=random_state\n",
        ")"
      ],
      "metadata": {
        "id": "cfBmqyJb3C12"
      },
      "id": "cfBmqyJb3C12",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Transformers"
      ],
      "metadata": {
        "id": "ye1oZZGVBKtE"
      },
      "id": "ye1oZZGVBKtE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned before, Pipelies are objects which chains together operations into one pipe. According to the documentation:  \n",
        "\n",
        "> Intermediate steps of the pipeline must be [**transforms**](https://scikit-learn.org/stable/glossary.html#term-transformer), that is, they must implement `fit()` and `transform()` methods.\n",
        "\n",
        "If we want to combine various operations performed on EEG data with pipelines, we need to wrap these operations in Transformer classes that implement `fit()` and `transform()` methods."
      ],
      "metadata": {
        "id": "KwCdia7WBkpJ"
      },
      "id": "KwCdia7WBkpJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# transformer for channel picking\n",
        "\n",
        "class PickChannels(TransformerMixin, BaseEstimator):\n",
        "    def __init__(self, channels_list=['Fz']):\n",
        "        self.channels_list = channels_list\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "\n",
        "        # apply pick_channel method from MNE to element\n",
        "        pick_channels_ = lambda x: x.pick_channels(ch_names=self.channels_list, ordered=True)\n",
        "\n",
        "        if not isinstance(X, np.ndarray):\n",
        "          raise TypeError(\"Wrong data fromat. Require: ndarray \")\n",
        "\n",
        "        # ensure shape of (n_samples,)\n",
        "        X = X.flatten()\n",
        "        \n",
        "        # for each element of list X apply (custom) function pick_channels_\n",
        "        # in other words - loop over the X applying function pick_channels_\n",
        "        X = np.array(list(map(pick_channels_, X)))\n",
        "\n",
        "        return X    "
      ],
      "metadata": {
        "id": "qv1cVmuIbftT"
      },
      "id": "qv1cVmuIbftT",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformer for epoch trimming\n",
        "\n",
        "class TrimEpoch(TransformerMixin, BaseEstimator):\n",
        "    def __init__(self, tmin=None, tmax=None, include_tmax=True, verbose=None):\n",
        "        self.tmin = tmin\n",
        "        self.tmax = tmax\n",
        "        self.include_tmax = include_tmax\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "\n",
        "        trim_epoch_ = lambda x: x.crop(\n",
        "            tmin = self.tmin, \n",
        "            tmax = self.tmax, \n",
        "            include_tmax = self.include_tmax, \n",
        "            verbose=self.verbose\n",
        "        )\n",
        "\n",
        "        if not isinstance(X, np.ndarray):\n",
        "          raise TypeError(\"Wrong data fromat. Require: ndarray \")\n",
        "\n",
        "        X = X.flatten()\n",
        "        X = np.array(list(map(trim_epoch_, X)))\n",
        "       \n",
        "        return X"
      ],
      "metadata": {
        "id": "t2L-qUEMOkAw"
      },
      "id": "t2L-qUEMOkAw",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformer for selecting events\n",
        "\n",
        "class SelectEvent(TransformerMixin, BaseEstimator):\n",
        "    def __init__(self, event_id = ['error_response']):\n",
        "        self.event_id = event_id\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "\n",
        "        select_events_ = lambda x: x[self.event_id]\n",
        "\n",
        "        if not isinstance(X, np.ndarray):\n",
        "          raise TypeError(\"Wrong data fromat. Require: ndarray \")\n",
        "\n",
        "        X = X.flatten()\n",
        "        X = np.array(list(map(select_events_, X)))\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "6RhOPaaoSfa8"
      },
      "id": "6RhOPaaoSfa8",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformer for evoking\n",
        "\n",
        "class Evoked(TransformerMixin, BaseEstimator):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        average_ = lambda x: x.average()\n",
        "\n",
        "        if not isinstance(X, np.ndarray):\n",
        "          raise TypeError(\"Wrong data fromat. Require: ndarray \")\n",
        "\n",
        "        X = X.flatten()\n",
        "        X = np.array(list(map(average_, X)))\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "izuEH-xMQL1R"
      },
      "id": "izuEH-xMQL1R",
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformer for getting data\n",
        "\n",
        "class ExtractData(TransformerMixin, BaseEstimator):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        # return shape of (n_channels, n_timepoints)\n",
        "        get_data_ = lambda x: x.get_data()\n",
        "\n",
        "        if not isinstance(X, np.ndarray):\n",
        "          raise TypeError(\"Wrong data fromat. Require: ndarray \")\n",
        "\n",
        "        X = X.flatten()\n",
        "        X = np.array(list(map(get_data_, X)))\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "CSMb48RzT-by"
      },
      "id": "CSMb48RzT-by",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformer for mean amplitude in time-window\n",
        "\n",
        "class MeanAmplitudePerChannel(TransformerMixin, BaseEstimator):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "      \n",
        "      if not isinstance(X, np.ndarray):\n",
        "          raise TypeError(\"Wrong data fromat. Require: ndarray \")\n",
        "\n",
        "      X = np.mean(X, axis=-1)\n",
        "\n",
        "      return X"
      ],
      "metadata": {
        "id": "2AM_L0Qrkb_A"
      },
      "id": "2AM_L0Qrkb_A",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformer for reshaping to (n_samples, n_features)\n",
        "\n",
        "class Reshape(TransformerMixin, BaseEstimator):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        if not isinstance(X, np.ndarray):\n",
        "          raise TypeError(\"Wrong data fromat. Require: ndarray \")\n",
        "\n",
        "        # reshape to (n_samples, n_features)\n",
        "        X = X.reshape(n_samples, -1)\n",
        "       \n",
        "        return X"
      ],
      "metadata": {
        "id": "32XUM3R2YYgN"
      },
      "id": "32XUM3R2YYgN",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformer for spatial filter reshape\n",
        "# require 3-D data: epochs x channels x timepoints\n",
        "\n",
        "class SpatialFilterPreprocessing(TransformerMixin, BaseEstimator):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "      \n",
        "      if not isinstance(X, np.ndarray):\n",
        "        raise TypeError(\"Wrong data fromat. Require: ndarray \")\n",
        "\n",
        "      # join data from each epoch. Shape: channels (n_features) x timepoints*epochs (n_samples)\n",
        "      timepoints_per_channel = np.concatenate(X, axis=1)\n",
        "\n",
        "      # create input vector for spatial filter training: array-like of shape (n_samples, n_features)\n",
        "      spatial_filter_input_data = timepoints_per_channel.T\n",
        "\n",
        "      return spatial_filter_input_data"
      ],
      "metadata": {
        "id": "fQAOdpxQPMPM"
      },
      "id": "fQAOdpxQPMPM",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X in spatial-filter shape: n_samples x n_features\n",
        "# Recovers shape: epoch x channel(spatial_filter_component) x timepoints\n",
        "class SpatialFilterPostprocessing(TransformerMixin, BaseEstimator):\n",
        "    def __init__(self, timepoints_count):\n",
        "        super().__init__()\n",
        "        self.timepoints_count = timepoints_count\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "      \n",
        "      if not isinstance(X, np.ndarray):\n",
        "        raise TypeError(\"Wrong data fromat. Require: ndarray \")\n",
        "\n",
        "      # print(f\"AFTER SPOC SHAPE:{X.shape}\")\n",
        "      # reshape to n_features x n_samples\n",
        "      X_transposed = X.T\n",
        "\n",
        "      # get number of created components(n_features)\n",
        "      spatial_filter_n_components = X.shape[1]\n",
        "\n",
        "      # get number of epochs: n_samples = epochs*timepoints -> epochs = n_samples / timepoints\n",
        "      n_epochs = int(X.shape[0] / self.timepoints_count)\n",
        "\n",
        "      # retrieve shape of epochs x n_components x timepoints\n",
        "      data_channel_wise = X_transposed.reshape(\n",
        "          spatial_filter_n_components, n_epochs, self.timepoints_count\n",
        "      )\n",
        "      data_epoch_wise = np.transpose(data_channel_wise, (1, 0, 2))\n",
        "\n",
        "      return np.array(data_epoch_wise)"
      ],
      "metadata": {
        "id": "uVmWroBJPWfI"
      },
      "id": "uVmWroBJPWfI",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PeakToPeak(TransformerMixin, BaseEstimator):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "\n",
        "      if not isinstance(X, np.ndarray):\n",
        "        raise TypeError(\"Wrong data fromat. Require: ndarray \")\n",
        "       # TODO: search for the highest and the lowest values in each channel for each participant\n",
        "\n",
        "      return X"
      ],
      "metadata": {
        "id": "VWXx2oOKWKYM"
      },
      "id": "VWXx2oOKWKYM",
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bin(TransformerMixin, BaseEstimator):\n",
        "    def __init__(self, bin_width=0.05):\n",
        "        super().__init__()\n",
        "        self.bin_width = bin_width\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "\n",
        "      if not isinstance(X, np.ndarray):\n",
        "        raise TypeError(\"Wrong data fromat. Require: ndarray \")\n",
        "        \n",
        "       # TODO: for each channel and each participant average the signal within bin_width time window.\n",
        "       # Assume that bin_width parameter is in s.\n",
        "\n",
        "      return X"
      ],
      "metadata": {
        "id": "eLkAcRV9WKct"
      },
      "id": "eLkAcRV9WKct",
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method for model evaluation"
      ],
      "metadata": {
        "id": "KrcHqgUtFiRo"
      },
      "id": "KrcHqgUtFiRo"
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_grid_search(\n",
        "    pipe,\n",
        "    X_train, \n",
        "    y_train, \n",
        "    X_test, \n",
        "    y_test, \n",
        "    regressor_params,\n",
        "    pipeline_name,\n",
        "    cv=KFold(n_splits=3),\n",
        "    predict_test = True,\n",
        "    predict_train = True,\n",
        "    ):\n",
        "    \n",
        "    # define grid search\n",
        "    grid_search_model = GridSearchCV(\n",
        "        pipe,\n",
        "        regressor_params,\n",
        "        cv=cv,\n",
        "        scoring={\"r2\", \"neg_mean_absolute_error\", \"neg_mean_squared_error\"},\n",
        "        refit=\"r2\",\n",
        "        return_train_score=True,\n",
        "        verbose=3,\n",
        "    )\n",
        "\n",
        "    # fit model\n",
        "    grid_search_model.fit(X_train, y_train)\n",
        "\n",
        "    # predict test data\n",
        "    y_test_pred = grid_search_model.predict(X_test) if predict_test is True else None\n",
        "    test_score = r2_score(y_test, y_test_pred) if predict_test is True else None\n",
        "\n",
        "    # predict train data\n",
        "    y_train_pred = grid_search_model.predict(X_train) if predict_train is True else None\n",
        "    train_score = r2_score(y_train, y_train_pred) if predict_train is True else None \n",
        "\n",
        "    # extract mean cv scores\n",
        "    mean_cv_score = grid_search_model.best_score_\n",
        "\n",
        "    # extract splits scores\n",
        "    cv_results_df = pd.DataFrame(grid_search_model.cv_results_).iloc[[grid_search_model.best_index_]]\n",
        "    cv_splits_scores_df = cv_results_df.filter(regex=r\"split\\d*_test_r2\").reset_index(drop=True)\n",
        "\n",
        "    # save results in dataframe\n",
        "    this_result = pd.concat(\n",
        "        [\n",
        "            pd.DataFrame({\n",
        "            \"model_name\": [pipe.steps[-1][0]],\n",
        "            \"pipeline_name\": [pipeline_name],\n",
        "            \"train score\": [train_score],\n",
        "            \"mean_cv_score\": [mean_cv_score],\n",
        "            \"test_score\": [test_score],\n",
        "            \"best_model\": [grid_search_model.best_estimator_],\n",
        "            \"parameters\": [grid_search_model.best_params_],\n",
        "            }),\n",
        "         cv_splits_scores_df\n",
        "        ],\n",
        "    axis=1\n",
        "    ) \n",
        "\n",
        "    return this_result"
      ],
      "metadata": {
        "id": "2x0IJaIOFmp-"
      },
      "id": "2x0IJaIOFmp-",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models to fit"
      ],
      "metadata": {
        "id": "ijjTS-VNFGE6"
      },
      "id": "ijjTS-VNFGE6"
    },
    {
      "cell_type": "code",
      "source": [
        "ln = (\"ln\", LinearRegression())\n",
        "ln_params = dict()\n",
        "\n",
        "svr = (\"svr\", SVR())\n",
        "svr_params = dict(\n",
        "    svr__kernel=[\"linear\", \"rbf\"],\n",
        "    svr__C=[0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 20, 50, 100],\n",
        "    svr__epsilon=[0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 2, 5, 10],\n",
        ")\n",
        "\n",
        "knn = (\"knn\", KNeighborsRegressor())\n",
        "knn_params = dict(\n",
        "    knn__n_neighbors=np.arange(1,50,2),\n",
        "    knn__weights=['uniform', 'distance']\n",
        ")\n",
        "\n",
        "estimators = [\n",
        "    (ln, ln_params),\n",
        "    (svr, svr_params),\n",
        "    (knn, knn_params),\n",
        "]"
      ],
      "metadata": {
        "id": "--lRxf9mBTWq"
      },
      "id": "--lRxf9mBTWq",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's think how we can featurise our EEG signal :)"
      ],
      "metadata": {
        "id": "oPTTCziedb6U"
      },
      "id": "oPTTCziedb6U"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ideas:\n",
        "- ...\n",
        "- ...\n",
        "- ..."
      ],
      "metadata": {
        "id": "aeJMuisvdwL-"
      },
      "id": "aeJMuisvdwL-"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Th3vxExXduIl"
      },
      "id": "Th3vxExXduIl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Z5mn_tUduMQ"
      },
      "id": "3Z5mn_tUduMQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W-WPNlemduPM"
      },
      "id": "W-WPNlemduPM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "107I_mlXduRu"
      },
      "id": "107I_mlXduRu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1vzF22VBduUm"
      },
      "id": "1vzF22VBduUm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My pre-defined Pipelines :)"
      ],
      "metadata": {
        "id": "gVlx-LVtFxyR"
      },
      "id": "gVlx-LVtFxyR"
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame()"
      ],
      "metadata": {
        "id": "iWB8u47vFxAw"
      },
      "id": "iWB8u47vFxAw",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline 1 - channels & mean amplitude:\n",
        "- channels: FCz\n",
        "- time-window: 0-100ms\n",
        "- feature: mean amplitude"
      ],
      "metadata": {
        "id": "Lv_loyDKBXpv"
      },
      "id": "Lv_loyDKBXpv"
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_name = \"FCz_0-100_mean_amplitude\""
      ],
      "metadata": {
        "id": "2yg0mSexVZNP"
      },
      "id": "2yg0mSexVZNP",
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create copy of X for multiple pre-processing"
      ],
      "metadata": {
        "id": "H2lxwZVYD0mM"
      },
      "id": "H2lxwZVYD0mM"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_copy = pd.DataFrame(copy.deepcopy(X_train.to_dict()))\n",
        "X_test_copy = pd.DataFrame(copy.deepcopy(X_test.to_dict()))\n",
        "\n",
        "# extract epochs from df to ndarray\n",
        "X_train_copy = X_train_copy.to_numpy()\n",
        "X_test_copy = X_test_copy.to_numpy()"
      ],
      "metadata": {
        "id": "bbxafY3fI8gd"
      },
      "execution_count": 81,
      "outputs": [],
      "id": "bbxafY3fI8gd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. To speed-up computations, those steps that do not need be optimised might be performed before grid search. These are ours base steps:"
      ],
      "metadata": {
        "id": "LYgwlXvWVcLS"
      },
      "id": "LYgwlXvWVcLS"
    },
    {
      "cell_type": "code",
      "source": [
        "# define base steps\n",
        "\n",
        "base_steps = [\n",
        "      ('pick_channel', PickChannels(['FCz'])),\n",
        "      ('trim_epoch', TrimEpoch(tmin=0, tmax=0.1)),\n",
        "      ('select_event', SelectEvent(event_id=['error_response'])),\n",
        "      ('evoked', Evoked()),\n",
        "      ('get_data', ExtractData()),\n",
        "      ('avg', MeanAmplitudePerChannel()),\n",
        "      ('reshape', Reshape()),\n",
        "      ('scaler', StandardScaler()), \n",
        "]\n",
        "\n",
        "base_pipeline = Pipeline(base_steps)"
      ],
      "metadata": {
        "id": "gyoN9-kCVV89"
      },
      "id": "gyoN9-kCVV89",
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform X train and test with base pipeline\n",
        "\n",
        "X_train_copy_transformed = base_pipeline.fit_transform(X_train_copy)\n",
        "X_test_copy_transformed = base_pipeline.transform(X_test_copy)\n",
        "\n",
        "print(f\"X train set shape: {X_train_copy_transformed.shape}\")\n",
        "print(f\"X test set shape: {X_test_copy_transformed.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaPMw2CwG-d9",
        "outputId": "55c24b7f-aff6-4923-c427-09af7847d8c5"
      },
      "id": "YaPMw2CwG-d9",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X train set shape: (96, 2)\n",
            "X test set shape: (42, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Evaluate models"
      ],
      "metadata": {
        "id": "_gKSfJBlVvmy"
      },
      "id": "_gKSfJBlVvmy"
    },
    {
      "cell_type": "code",
      "source": [
        "for (estimator, params) in estimators:\n",
        "  print(f\"Rating {estimator} \\n\")\n",
        "\n",
        "  # create pipeline from base steps list and estimator\n",
        "  pipe = Pipeline([estimator])\n",
        "\n",
        "  # enter to grid search\n",
        "  this_results = evaluate_model_grid_search(\n",
        "      pipe,\n",
        "      X_train_copy_transformed,\n",
        "      y_train,\n",
        "      X_test_copy_transformed,\n",
        "      y_test,\n",
        "      params,\n",
        "      pipeline_name=pipeline_name,\n",
        "      predict_test = True,\n",
        "      predict_train = True,\n",
        "  )\n",
        "\n",
        "  results_df = results_df.append(this_results)"
      ],
      "metadata": {
        "id": "GdJXXwrpFvVr"
      },
      "id": "GdJXXwrpFvVr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "R3yz1AFCO3-S"
      },
      "id": "R3yz1AFCO3-S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline 2 - channels & peak-to-peak amplitude: \n",
        "- channels: Fz\n",
        "- time-window: 0-100ms\n",
        "- feature: peak-to-peak amplitude"
      ],
      "metadata": {
        "id": "cJUDScHeGUNm"
      },
      "id": "cJUDScHeGUNm"
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_name = \"Fz_0-100_peak_to_peak\""
      ],
      "metadata": {
        "id": "GdR3JVU7V4IQ"
      },
      "id": "GdR3JVU7V4IQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create copy of X for multiple pre-processing"
      ],
      "metadata": {
        "id": "2l8mIRrRI2Jv"
      },
      "id": "2l8mIRrRI2Jv"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_copy = pd.DataFrame(copy.deepcopy(X_train.to_dict()))\n",
        "X_test_copy = pd.DataFrame(copy.deepcopy(X_test.to_dict()))\n",
        "\n",
        "# extract epochs from df to ndarray\n",
        "X_train_copy = X_train_copy.to_numpy()\n",
        "X_test_copy = X_test_copy.to_numpy()"
      ],
      "metadata": {
        "id": "cLDzOLtCI2Jv"
      },
      "execution_count": 141,
      "outputs": [],
      "id": "cLDzOLtCI2Jv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. To speed-up computations, those steps that do not need be optimised might be performed before grid search. These are ours base steps:"
      ],
      "metadata": {
        "id": "dBjBGe4sV2PT"
      },
      "id": "dBjBGe4sV2PT"
    },
    {
      "cell_type": "code",
      "source": [
        "# define base steps\n",
        "\n",
        "base_steps = [\n",
        "      ('pick_channel', PickChannels(['Fz'])),\n",
        "      ('trim_epoch', TrimEpoch(tmin=0, tmax=0.1)),\n",
        "      ('select_event', SelectEvent(event_id=['error_response'])),\n",
        "      ('evoked', Evoked()),\n",
        "      ('get_data', ExtractData()),\n",
        "      # ('peak-to-peak', PeakToPeak()), # TODO\n",
        "      ('reshape', Reshape()),\n",
        "      ('scaler', StandardScaler()), \n",
        "]\n",
        "\n",
        "base_pipeline = Pipeline(base_steps)"
      ],
      "metadata": {
        "id": "WxS5j8VRI2Jv"
      },
      "execution_count": 79,
      "outputs": [],
      "id": "WxS5j8VRI2Jv"
    },
    {
      "cell_type": "code",
      "source": [
        "# transform X train and test with base pipeline\n",
        "\n",
        "X_train_copy_transformed = base_pipeline.fit_transform(X_train_copy)\n",
        "X_test_copy_transformed = base_pipeline.transform(X_test_copy)\n",
        "\n",
        "print(f\"X train set shape: {X_train_copy_transformed.shape}\")\n",
        "print(f\"X test set shape: {X_test_copy_transformed.shape}\")"
      ],
      "metadata": {
        "id": "RorVPZDMI2Jx"
      },
      "execution_count": 80,
      "outputs": [],
      "id": "RorVPZDMI2Jx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Evaluate model"
      ],
      "metadata": {
        "id": "bkYF0GUfWBD1"
      },
      "id": "bkYF0GUfWBD1"
    },
    {
      "cell_type": "code",
      "source": [
        "for (estimator, params) in estimators:\n",
        "  print(f\"Rating {estimator} \\n\")\n",
        "\n",
        "  # create pipeline from base steps list and estimator\n",
        "  pipe = Pipeline([estimator])\n",
        "\n",
        "  # enter to grid search\n",
        "  this_results = evaluate_model_grid_search(\n",
        "      pipe,\n",
        "      X_train_copy_transformed,\n",
        "      y_train,\n",
        "      X_test_copy_transformed,\n",
        "      y_test,\n",
        "      params,\n",
        "      pipeline_name=pipeline_name,\n",
        "      predict_test = True,\n",
        "      predict_train = True,\n",
        "  )\n",
        "\n",
        "  results_df = results_df.append(this_results)"
      ],
      "metadata": {
        "id": "q4JMOp-lI2Jy"
      },
      "execution_count": null,
      "outputs": [],
      "id": "q4JMOp-lI2Jy"
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "outputId": "5a88effc-3e2f-400f-e9b9-8bea0a530032",
        "id": "zP4XoOm0I2Jy"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  model_name             pipeline_name  train score  mean_cv_score  \\\n",
              "0         ln  FCz_0-100_mean_amplitude     0.022035      -0.140625   \n",
              "0        svr  FCz_0-100_mean_amplitude    -0.002543      -0.077124   \n",
              "0        knn  FCz_0-100_mean_amplitude     0.000840      -0.113897   \n",
              "0         ln     Fz_0-100_peak_to_peak     0.265114      -1.039627   \n",
              "0        svr     Fz_0-100_peak_to_peak    -0.000041      -0.089083   \n",
              "0        knn     Fz_0-100_peak_to_peak     0.019391      -0.115004   \n",
              "\n",
              "   test_score                              best_model  \\\n",
              "0    0.049269                    (LinearRegression())   \n",
              "0    0.093865  (SVR(C=1, epsilon=2, kernel='linear'))   \n",
              "0   -0.014011   (KNeighborsRegressor(n_neighbors=49))   \n",
              "0    0.006122                    (LinearRegression())   \n",
              "0   -0.008669               (SVR(C=1e-05, epsilon=1))   \n",
              "0    0.046520   (KNeighborsRegressor(n_neighbors=31))   \n",
              "\n",
              "                                          parameters  split0_test_r2  \\\n",
              "0                                                 {}       -0.088205   \n",
              "0  {'svr__C': 1, 'svr__epsilon': 2, 'svr__kernel'...       -0.125013   \n",
              "0  {'knn__n_neighbors': 49, 'knn__weights': 'unif...       -0.107084   \n",
              "0                                                 {}       -0.863369   \n",
              "0  {'svr__C': 1e-05, 'svr__epsilon': 1, 'svr__ker...       -0.107622   \n",
              "0  {'knn__n_neighbors': 31, 'knn__weights': 'unif...       -0.084702   \n",
              "\n",
              "   split1_test_r2  split2_test_r2  \n",
              "0       -0.354046        0.020376  \n",
              "0       -0.206596        0.100237  \n",
              "0       -0.248381        0.013775  \n",
              "0       -1.213246       -1.042266  \n",
              "0       -0.158919       -0.000709  \n",
              "0       -0.287861        0.027552  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3c6c51ed-c8f0-45e3-a358-afb9ab2857ff\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>pipeline_name</th>\n",
              "      <th>train score</th>\n",
              "      <th>mean_cv_score</th>\n",
              "      <th>test_score</th>\n",
              "      <th>best_model</th>\n",
              "      <th>parameters</th>\n",
              "      <th>split0_test_r2</th>\n",
              "      <th>split1_test_r2</th>\n",
              "      <th>split2_test_r2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ln</td>\n",
              "      <td>FCz_0-100_mean_amplitude</td>\n",
              "      <td>0.022035</td>\n",
              "      <td>-0.140625</td>\n",
              "      <td>0.049269</td>\n",
              "      <td>(LinearRegression())</td>\n",
              "      <td>{}</td>\n",
              "      <td>-0.088205</td>\n",
              "      <td>-0.354046</td>\n",
              "      <td>0.020376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>svr</td>\n",
              "      <td>FCz_0-100_mean_amplitude</td>\n",
              "      <td>-0.002543</td>\n",
              "      <td>-0.077124</td>\n",
              "      <td>0.093865</td>\n",
              "      <td>(SVR(C=1, epsilon=2, kernel='linear'))</td>\n",
              "      <td>{'svr__C': 1, 'svr__epsilon': 2, 'svr__kernel'...</td>\n",
              "      <td>-0.125013</td>\n",
              "      <td>-0.206596</td>\n",
              "      <td>0.100237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>knn</td>\n",
              "      <td>FCz_0-100_mean_amplitude</td>\n",
              "      <td>0.000840</td>\n",
              "      <td>-0.113897</td>\n",
              "      <td>-0.014011</td>\n",
              "      <td>(KNeighborsRegressor(n_neighbors=49))</td>\n",
              "      <td>{'knn__n_neighbors': 49, 'knn__weights': 'unif...</td>\n",
              "      <td>-0.107084</td>\n",
              "      <td>-0.248381</td>\n",
              "      <td>0.013775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ln</td>\n",
              "      <td>Fz_0-100_peak_to_peak</td>\n",
              "      <td>0.265114</td>\n",
              "      <td>-1.039627</td>\n",
              "      <td>0.006122</td>\n",
              "      <td>(LinearRegression())</td>\n",
              "      <td>{}</td>\n",
              "      <td>-0.863369</td>\n",
              "      <td>-1.213246</td>\n",
              "      <td>-1.042266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>svr</td>\n",
              "      <td>Fz_0-100_peak_to_peak</td>\n",
              "      <td>-0.000041</td>\n",
              "      <td>-0.089083</td>\n",
              "      <td>-0.008669</td>\n",
              "      <td>(SVR(C=1e-05, epsilon=1))</td>\n",
              "      <td>{'svr__C': 1e-05, 'svr__epsilon': 1, 'svr__ker...</td>\n",
              "      <td>-0.107622</td>\n",
              "      <td>-0.158919</td>\n",
              "      <td>-0.000709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>knn</td>\n",
              "      <td>Fz_0-100_peak_to_peak</td>\n",
              "      <td>0.019391</td>\n",
              "      <td>-0.115004</td>\n",
              "      <td>0.046520</td>\n",
              "      <td>(KNeighborsRegressor(n_neighbors=31))</td>\n",
              "      <td>{'knn__n_neighbors': 31, 'knn__weights': 'unif...</td>\n",
              "      <td>-0.084702</td>\n",
              "      <td>-0.287861</td>\n",
              "      <td>0.027552</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c6c51ed-c8f0-45e3-a358-afb9ab2857ff')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3c6c51ed-c8f0-45e3-a358-afb9ab2857ff button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3c6c51ed-c8f0-45e3-a358-afb9ab2857ff');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "id": "zP4XoOm0I2Jy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline 3 - PCA: \n",
        "- channels: F1, Fz, F2, FC1, FCz, FC2, C1, Cz, C2, CP1, CPz, CP2;\n",
        "- time-window: 0-100ms\n",
        "- PCA\n",
        "- feature: mean amplitude/peak-to-peak"
      ],
      "metadata": {
        "id": "8qWHxDCvGjfV"
      },
      "id": "8qWHxDCvGjfV"
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_name = \"PCA_0-100_mean_amplitude\""
      ],
      "metadata": {
        "id": "zcrXU70gUqFD"
      },
      "id": "zcrXU70gUqFD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create copy of X for multiple pre-processing"
      ],
      "metadata": {
        "id": "C3XAdJGSJ7Lr"
      },
      "id": "C3XAdJGSJ7Lr"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_copy = pd.DataFrame(copy.deepcopy(X_train.to_dict()))\n",
        "X_test_copy = pd.DataFrame(copy.deepcopy(X_test.to_dict()))\n",
        "\n",
        "# extract epochs from df to ndarray\n",
        "X_train_copy = X_train_copy.to_numpy()\n",
        "X_test_copy = X_test_copy.to_numpy()"
      ],
      "metadata": {
        "id": "MY-23IX_J7Ls"
      },
      "execution_count": 160,
      "outputs": [],
      "id": "MY-23IX_J7Ls"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. To speed-up computations, those steps that do not need be optimised might be performed before grid search. These are ours base steps:"
      ],
      "metadata": {
        "id": "53KNnMWUUchv"
      },
      "id": "53KNnMWUUchv"
    },
    {
      "cell_type": "code",
      "source": [
        "# define base steps\n",
        "channels = ['F1', 'Fz', 'F2', 'FC1', 'FCz', 'FC2', 'C1', 'Cz', 'C2', 'CP1', 'CPz', 'CP2']\n",
        "\n",
        "base_steps = [\n",
        "      ('pick_channel', PickChannels(channels)),\n",
        "      ('trim_epoch', TrimEpoch(tmin=0, tmax=0.2)),\n",
        "      ('select_event', SelectEvent(event_id=['error_response'])),\n",
        "      ('evoked', Evoked()),\n",
        "      ('get_data', ExtractData()),\n",
        "]\n",
        "\n",
        "# create base pipeline\n",
        "base_pipeline = Pipeline(base_steps)"
      ],
      "metadata": {
        "id": "KO2j4og6J7Ls"
      },
      "execution_count": 162,
      "outputs": [],
      "id": "KO2j4og6J7Ls"
    },
    {
      "cell_type": "code",
      "source": [
        "# transform X train and test with base pipeline\n",
        "X_train_copy_transformed = base_pipeline.fit_transform(X_train_copy)\n",
        "X_test_copy_transformed = base_pipeline.transform(X_test_copy)\n",
        "\n",
        "print(f\"X train set shape: {X_train_copy_transformed.shape}\")\n",
        "print(f\"X test set shape: {X_test_copy_transformed.shape}\")"
      ],
      "metadata": {
        "id": "KVl6vA4eJ7Ls"
      },
      "execution_count": 163,
      "outputs": [],
      "id": "KVl6vA4eJ7Ls"
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create main pre-processign pipeline:"
      ],
      "metadata": {
        "id": "J9frmVdNSOb8"
      },
      "id": "J9frmVdNSOb8"
    },
    {
      "cell_type": "code",
      "source": [
        "timepoints_count = X_train_copy_transformed.shape[-1]\n",
        "\n",
        "steps =[\n",
        "    ('reshape_for_PCA', SpatialFilterPreprocessing()),\n",
        "    (\"spatial_feature_extraction\",PCA(random_state=random_state)),\n",
        "    (\"reshape_after_PCA\", SpatialFilterPostprocessing(timepoints_count=timepoints_count)),\n",
        "    ('avg', MeanAmplitudePerChannel()),\n",
        "    ('reshape', Reshape()),\n",
        "    ('scaler', StandardScaler()), \n",
        "]"
      ],
      "metadata": {
        "id": "Yxu5BRo5R8lb"
      },
      "id": "Yxu5BRo5R8lb",
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define parameters of the main pipeline\n",
        "pipeline_params = dict(\n",
        "    spatial_feature_extraction__n_components = np.arange(1,5)\n",
        ")"
      ],
      "metadata": {
        "id": "3G8KduEcToq2"
      },
      "id": "3G8KduEcToq2",
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Evaluate model"
      ],
      "metadata": {
        "id": "THer08uzVss4"
      },
      "id": "THer08uzVss4"
    },
    {
      "cell_type": "code",
      "source": [
        "for (estimator, params) in estimators:\n",
        "  print(f\"Rating {estimator} \\n\")\n",
        "\n",
        "  # create pipeline from base steps list and estimator\n",
        "  pipe = Pipeline(steps + [estimator])\n",
        "\n",
        "  # create params: join estimator parameters and main pipeline parameters\n",
        "  parameters = params.copy()\n",
        "  parameters.update(pipeline_params)\n",
        "\n",
        "  # enter to grid search\n",
        "  this_results = evaluate_model_grid_search(\n",
        "      pipe,\n",
        "      X_train_copy_transformed,\n",
        "      y_train,\n",
        "      X_test_copy_transformed,\n",
        "      y_test,\n",
        "      parameters,\n",
        "      pipeline_name=pipeline_name,\n",
        "      predict_test = True,\n",
        "      predict_train = True,\n",
        "  )\n",
        "\n",
        "  results_df = results_df.append(this_results)"
      ],
      "metadata": {
        "id": "TekBjYH1J7Lt"
      },
      "execution_count": null,
      "outputs": [],
      "id": "TekBjYH1J7Lt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline 4 - channels & bins: \n",
        "- channels: Fz\n",
        "- time-window: 0-100ms\n",
        "- 50ms bins\n",
        "- feature: peak-to-peak amplitude"
      ],
      "metadata": {
        "id": "fqLoHKoUGu4t"
      },
      "id": "fqLoHKoUGu4t"
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_name = \"Fz_0-100_bins_peak_to_peak\""
      ],
      "metadata": {
        "id": "GEf95nSNbkp7"
      },
      "execution_count": null,
      "outputs": [],
      "id": "GEf95nSNbkp7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create copy of X for multiple pre-processing"
      ],
      "metadata": {
        "id": "yxHy4TCSbkp7"
      },
      "id": "yxHy4TCSbkp7"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_copy = pd.DataFrame(copy.deepcopy(X_train.to_dict()))\n",
        "X_test_copy = pd.DataFrame(copy.deepcopy(X_test.to_dict()))\n",
        "\n",
        "# extract epochs from df to ndarray\n",
        "X_train_copy = X_train_copy.to_numpy()\n",
        "X_test_copy = X_test_copy.to_numpy()"
      ],
      "metadata": {
        "id": "WrXzR38abkp7"
      },
      "execution_count": null,
      "outputs": [],
      "id": "WrXzR38abkp7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. To speed-up computations, those steps that do not need be optimised might be performed before grid search. These are ours base steps:"
      ],
      "metadata": {
        "id": "oOP9FWfIbkp7"
      },
      "id": "oOP9FWfIbkp7"
    },
    {
      "cell_type": "code",
      "source": [
        "# define base steps\n",
        "\n",
        "base_steps = [\n",
        "      ('pick_channel', PickChannels(['Fz'])),\n",
        "      ('trim_epoch', TrimEpoch(tmin=0, tmax=0.1)),\n",
        "      ('select_event', SelectEvent(event_id=['error_response'])),\n",
        "      ('evoked', Evoked()),\n",
        "      ('get_data', ExtractData()),\n",
        "      # ('bin', Bin()), # TODO\n",
        "      # ('peak-to-peak', PeakToPeak()), # TODO\n",
        "      ('reshape', Reshape()),\n",
        "      ('scaler', StandardScaler()), \n",
        "]\n",
        "\n",
        "base_pipeline = Pipeline(base_steps)"
      ],
      "metadata": {
        "id": "MPqWHmrWbkp7"
      },
      "execution_count": null,
      "outputs": [],
      "id": "MPqWHmrWbkp7"
    },
    {
      "cell_type": "code",
      "source": [
        "# transform X train and test with base pipeline\n",
        "\n",
        "X_train_copy_transformed = base_pipeline.fit_transform(X_train_copy)\n",
        "X_test_copy_transformed = base_pipeline.transform(X_test_copy)\n",
        "\n",
        "print(f\"X train set shape: {X_train_copy_transformed.shape}\")\n",
        "print(f\"X test set shape: {X_test_copy_transformed.shape}\")"
      ],
      "metadata": {
        "id": "E0cMPUscbkp8"
      },
      "execution_count": null,
      "outputs": [],
      "id": "E0cMPUscbkp8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Evaluate model"
      ],
      "metadata": {
        "id": "p0tPqkMobkp8"
      },
      "id": "p0tPqkMobkp8"
    },
    {
      "cell_type": "code",
      "source": [
        "for (estimator, params) in estimators:\n",
        "  print(f\"Rating {estimator} \\n\")\n",
        "\n",
        "  # create pipeline from base steps list and estimator\n",
        "  pipe = Pipeline([estimator])\n",
        "\n",
        "  # enter to grid search\n",
        "  this_results = evaluate_model_grid_search(\n",
        "      pipe,\n",
        "      X_train_copy_transformed,\n",
        "      y_train,\n",
        "      X_test_copy_transformed,\n",
        "      y_test,\n",
        "      params,\n",
        "      pipeline_name=pipeline_name,\n",
        "      predict_test = True,\n",
        "      predict_train = True,\n",
        "  )\n",
        "\n",
        "  results_df = results_df.append(this_results)"
      ],
      "metadata": {
        "id": "gnIR_LJUbkp8"
      },
      "execution_count": null,
      "outputs": [],
      "id": "gnIR_LJUbkp8"
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "outputId": "5a88effc-3e2f-400f-e9b9-8bea0a530032",
        "id": "Aoyfkxnlbkp8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  model_name             pipeline_name  train score  mean_cv_score  \\\n",
              "0         ln  FCz_0-100_mean_amplitude     0.022035      -0.140625   \n",
              "0        svr  FCz_0-100_mean_amplitude    -0.002543      -0.077124   \n",
              "0        knn  FCz_0-100_mean_amplitude     0.000840      -0.113897   \n",
              "0         ln     Fz_0-100_peak_to_peak     0.265114      -1.039627   \n",
              "0        svr     Fz_0-100_peak_to_peak    -0.000041      -0.089083   \n",
              "0        knn     Fz_0-100_peak_to_peak     0.019391      -0.115004   \n",
              "\n",
              "   test_score                              best_model  \\\n",
              "0    0.049269                    (LinearRegression())   \n",
              "0    0.093865  (SVR(C=1, epsilon=2, kernel='linear'))   \n",
              "0   -0.014011   (KNeighborsRegressor(n_neighbors=49))   \n",
              "0    0.006122                    (LinearRegression())   \n",
              "0   -0.008669               (SVR(C=1e-05, epsilon=1))   \n",
              "0    0.046520   (KNeighborsRegressor(n_neighbors=31))   \n",
              "\n",
              "                                          parameters  split0_test_r2  \\\n",
              "0                                                 {}       -0.088205   \n",
              "0  {'svr__C': 1, 'svr__epsilon': 2, 'svr__kernel'...       -0.125013   \n",
              "0  {'knn__n_neighbors': 49, 'knn__weights': 'unif...       -0.107084   \n",
              "0                                                 {}       -0.863369   \n",
              "0  {'svr__C': 1e-05, 'svr__epsilon': 1, 'svr__ker...       -0.107622   \n",
              "0  {'knn__n_neighbors': 31, 'knn__weights': 'unif...       -0.084702   \n",
              "\n",
              "   split1_test_r2  split2_test_r2  \n",
              "0       -0.354046        0.020376  \n",
              "0       -0.206596        0.100237  \n",
              "0       -0.248381        0.013775  \n",
              "0       -1.213246       -1.042266  \n",
              "0       -0.158919       -0.000709  \n",
              "0       -0.287861        0.027552  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3c6c51ed-c8f0-45e3-a358-afb9ab2857ff\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>pipeline_name</th>\n",
              "      <th>train score</th>\n",
              "      <th>mean_cv_score</th>\n",
              "      <th>test_score</th>\n",
              "      <th>best_model</th>\n",
              "      <th>parameters</th>\n",
              "      <th>split0_test_r2</th>\n",
              "      <th>split1_test_r2</th>\n",
              "      <th>split2_test_r2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ln</td>\n",
              "      <td>FCz_0-100_mean_amplitude</td>\n",
              "      <td>0.022035</td>\n",
              "      <td>-0.140625</td>\n",
              "      <td>0.049269</td>\n",
              "      <td>(LinearRegression())</td>\n",
              "      <td>{}</td>\n",
              "      <td>-0.088205</td>\n",
              "      <td>-0.354046</td>\n",
              "      <td>0.020376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>svr</td>\n",
              "      <td>FCz_0-100_mean_amplitude</td>\n",
              "      <td>-0.002543</td>\n",
              "      <td>-0.077124</td>\n",
              "      <td>0.093865</td>\n",
              "      <td>(SVR(C=1, epsilon=2, kernel='linear'))</td>\n",
              "      <td>{'svr__C': 1, 'svr__epsilon': 2, 'svr__kernel'...</td>\n",
              "      <td>-0.125013</td>\n",
              "      <td>-0.206596</td>\n",
              "      <td>0.100237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>knn</td>\n",
              "      <td>FCz_0-100_mean_amplitude</td>\n",
              "      <td>0.000840</td>\n",
              "      <td>-0.113897</td>\n",
              "      <td>-0.014011</td>\n",
              "      <td>(KNeighborsRegressor(n_neighbors=49))</td>\n",
              "      <td>{'knn__n_neighbors': 49, 'knn__weights': 'unif...</td>\n",
              "      <td>-0.107084</td>\n",
              "      <td>-0.248381</td>\n",
              "      <td>0.013775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ln</td>\n",
              "      <td>Fz_0-100_peak_to_peak</td>\n",
              "      <td>0.265114</td>\n",
              "      <td>-1.039627</td>\n",
              "      <td>0.006122</td>\n",
              "      <td>(LinearRegression())</td>\n",
              "      <td>{}</td>\n",
              "      <td>-0.863369</td>\n",
              "      <td>-1.213246</td>\n",
              "      <td>-1.042266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>svr</td>\n",
              "      <td>Fz_0-100_peak_to_peak</td>\n",
              "      <td>-0.000041</td>\n",
              "      <td>-0.089083</td>\n",
              "      <td>-0.008669</td>\n",
              "      <td>(SVR(C=1e-05, epsilon=1))</td>\n",
              "      <td>{'svr__C': 1e-05, 'svr__epsilon': 1, 'svr__ker...</td>\n",
              "      <td>-0.107622</td>\n",
              "      <td>-0.158919</td>\n",
              "      <td>-0.000709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>knn</td>\n",
              "      <td>Fz_0-100_peak_to_peak</td>\n",
              "      <td>0.019391</td>\n",
              "      <td>-0.115004</td>\n",
              "      <td>0.046520</td>\n",
              "      <td>(KNeighborsRegressor(n_neighbors=31))</td>\n",
              "      <td>{'knn__n_neighbors': 31, 'knn__weights': 'unif...</td>\n",
              "      <td>-0.084702</td>\n",
              "      <td>-0.287861</td>\n",
              "      <td>0.027552</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c6c51ed-c8f0-45e3-a358-afb9ab2857ff')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3c6c51ed-c8f0-45e3-a358-afb9ab2857ff button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3c6c51ed-c8f0-45e3-a358-afb9ab2857ff');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "id": "Aoyfkxnlbkp8"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DLbcWIj1bhuz"
      },
      "id": "DLbcWIj1bhuz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline 5 - PCA & bins: \n",
        "- channels: F1, Fz, F2, FC1, FCz, FC2, C1, Cz, C2, CP1, CPz, CP2;\n",
        "- time-window: 0-100ms\n",
        "- PCA\n",
        "- 50ms bins\n",
        "- feature: mean/peak-to-peak amplitude"
      ],
      "metadata": {
        "id": "b_Thr7U3G3An"
      },
      "id": "b_Thr7U3G3An"
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_name = \"PCA_0-100_bins_mean_amplitude\""
      ],
      "metadata": {
        "id": "TFnj1I_ca32V"
      },
      "execution_count": null,
      "outputs": [],
      "id": "TFnj1I_ca32V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create copy of X for multiple pre-processing"
      ],
      "metadata": {
        "id": "mFF12M4_a32V"
      },
      "id": "mFF12M4_a32V"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_copy = pd.DataFrame(copy.deepcopy(X_train.to_dict()))\n",
        "X_test_copy = pd.DataFrame(copy.deepcopy(X_test.to_dict()))\n",
        "\n",
        "# extract epochs from df to ndarray\n",
        "X_train_copy = X_train_copy.to_numpy()\n",
        "X_test_copy = X_test_copy.to_numpy()"
      ],
      "metadata": {
        "id": "ploLfLpca32W"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ploLfLpca32W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. To speed-up computations, those steps that do not need be optimised might be performed before grid search. These are ours base steps:"
      ],
      "metadata": {
        "id": "uzWZQ48Ea32W"
      },
      "id": "uzWZQ48Ea32W"
    },
    {
      "cell_type": "code",
      "source": [
        "# define base steps\n",
        "channels = ['F1', 'Fz', 'F2', 'FC1', 'FCz', 'FC2', 'C1', 'Cz', 'C2', 'CP1', 'CPz', 'CP2']\n",
        "\n",
        "base_steps = [\n",
        "      ('pick_channel', PickChannels(channels)),\n",
        "      ('trim_epoch', TrimEpoch(tmin=0, tmax=0.2)),\n",
        "      ('select_event', SelectEvent(event_id=['error_response'])),\n",
        "      ('evoked', Evoked()),\n",
        "      ('get_data', ExtractData()),\n",
        "]\n",
        "\n",
        "# create base pipeline\n",
        "base_pipeline = Pipeline(base_steps)"
      ],
      "metadata": {
        "id": "ax_cCOV5a32W"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ax_cCOV5a32W"
    },
    {
      "cell_type": "code",
      "source": [
        "# transform X train and test with base pipeline\n",
        "X_train_copy_transformed = base_pipeline.fit_transform(X_train_copy)\n",
        "X_test_copy_transformed = base_pipeline.transform(X_test_copy)\n",
        "\n",
        "print(f\"X train set shape: {X_train_copy_transformed.shape}\")\n",
        "print(f\"X test set shape: {X_test_copy_transformed.shape}\")"
      ],
      "metadata": {
        "id": "6NGRmbH1a32W"
      },
      "execution_count": null,
      "outputs": [],
      "id": "6NGRmbH1a32W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create main pre-processign pipeline:"
      ],
      "metadata": {
        "id": "HBGQYdvVa32W"
      },
      "id": "HBGQYdvVa32W"
    },
    {
      "cell_type": "code",
      "source": [
        "timepoints_count = X_train_copy_transformed.shape[-1]\n",
        "\n",
        "steps =[\n",
        "    ('reshape_for_PCA', SpatialFilterPreprocessing()),\n",
        "    (\"spatial_feature_extraction\",PCA(random_state=random_state)),\n",
        "    (\"reshape_after_PCA\", SpatialFilterPostprocessing(timepoints_count=timepoints_count)),\n",
        "    # ('bin', Bin()), # TODO\n",
        "    ('avg', MeanAmplitudePerChannel()),\n",
        "    ('reshape', Reshape()),\n",
        "    ('scaler', StandardScaler()), \n",
        "]"
      ],
      "metadata": {
        "id": "xmWkvwmRa32W"
      },
      "execution_count": null,
      "outputs": [],
      "id": "xmWkvwmRa32W"
    },
    {
      "cell_type": "code",
      "source": [
        "# define parameters of the main pipeline\n",
        "pipeline_params = dict(\n",
        "    spatial_feature_extraction__n_components = np.arange(1,5)\n",
        ")"
      ],
      "metadata": {
        "id": "METj77O8a32W"
      },
      "execution_count": null,
      "outputs": [],
      "id": "METj77O8a32W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Evaluate model"
      ],
      "metadata": {
        "id": "-JS6Cqnxa32W"
      },
      "id": "-JS6Cqnxa32W"
    },
    {
      "cell_type": "code",
      "source": [
        "for (estimator, params) in estimators:\n",
        "  print(f\"Rating {estimator} \\n\")\n",
        "\n",
        "  # create pipeline from base steps list and estimator\n",
        "  pipe = Pipeline(steps + [estimator])\n",
        "\n",
        "  # create params: join estimator parameters and main pipeline parameters\n",
        "  parameters = params.copy()\n",
        "  parameters.update(pipeline_params)\n",
        "\n",
        "  # enter to grid search\n",
        "  this_results = evaluate_model_grid_search(\n",
        "      pipe,\n",
        "      X_train_copy_transformed,\n",
        "      y_train,\n",
        "      X_test_copy_transformed,\n",
        "      y_test,\n",
        "      parameters,\n",
        "      pipeline_name=pipeline_name,\n",
        "      predict_test = True,\n",
        "      predict_train = True,\n",
        "  )\n",
        "\n",
        "  results_df = results_df.append(this_results)"
      ],
      "metadata": {
        "id": "ephI01yFa32W"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ephI01yFa32W"
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "9KMYkVGkbP7K"
      },
      "id": "9KMYkVGkbP7K",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "erpinator",
      "language": "python",
      "name": "erpinator"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}